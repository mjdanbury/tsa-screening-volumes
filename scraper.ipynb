{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper.ipynb\n",
    "In this notebook, we scrape passenger volume data from the TSA website using `requests`, a library that allows python to get things from the internet, and `BeautifulSoup`, a library for extracting information from webpages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019\n",
    "When web scraping, its helpful to get your code working for one page before running it over multiple pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab a webpage from the internet using requests\n",
    "year = 2019\n",
    "url = \"https://www.tsa.gov/travel/passenger-volumes/\" + str(year)\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the webpages using BeautifulSoup\n",
    "doc = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you navigate to the webpage we're looking at in your browser. You'll see a data table. We want to get that data from the table and put it into a form we can analyze. To do that, we right-click on a cells in the table and choose 'inspect element'. That's what we do here. This pulls up a bunch of computery stuff in a part of your web browser you might not have ever used before. This is HTML - what the webpage looks like under the hood so the computer can understand it. \n",
    "\n",
    "\n",
    "If we look at the section that is highlighted, we see a section that says `class=\"views-field views-field-field-travel-number-date views-align-center\"`. It turns out that the `views-field-field-travel-number-date` is a piece of HTML markup that is unique to the dates in the table. We can use this uniqueness in our code to easily grab these dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab the \n",
    "dates = [date.text.strip() for date in doc.select('.views-field-field-travel-number-date')[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same inspect element process for the values, we see that we can use a similar tag `views-field views-field-field-travel-number` to get the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [int(number.text.strip().replace(',', '')) for number in doc.select('.views-field-field-travel-number')[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that's left to do is get this information into a form we can export it. We use the library `pandas` for writing data to csv files and pandas takes data in lists `[]` of dictionaries `{}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date': '1/1/2019', 'number': 2201765},\n",
       " {'date': '1/2/2019', 'number': 2424225},\n",
       " {'date': '1/3/2019', 'number': 2279384},\n",
       " {'date': '1/4/2019', 'number': 2230078},\n",
       " {'date': '1/5/2019', 'number': 2049460}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\"date\": date, \"number\": number} for (date, number) in zip(dates, numbers)][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the data\n",
    "Great! It looks like it worked for one page, now we just have to apply that for multiple pages. If we click around on the TSA website, we see that all the URL's (the things you type into the top of your browser) for all data pages for previous years are the same except for the year at the end (\"https://www.tsa.gov/travel/passenger-volumes/2019\", \"https://www.tsa.gov/travel/passenger-volumes/2020\", etc.)\n",
    "\n",
    "We could use the code we wrote above and just change the year by hand everytime, but we're busy people and coding allows us to do this automatically with a concept called a for loop. For loops in python are easy. `for year in range (2019,2024):` sets the variable `year` to the value `2019`, runs the indented code after the `:`, then repeats, chaning the value of `year` to `2020`, then `2021`, and so on and so forth. \n",
    "\n",
    "The indented code is exactly the same code we wrote before, except for `time.sleep(1)` which tells python to wait for one second between runs. This is considerd polite when web scraping and can help prevent the website from blocking your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2019,2024):\n",
    "    url = \"https://www.tsa.gov/travel/passenger-volumes/\" + str(year)\n",
    "    response = requests.get(url)\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    dates = [date.text.strip() for date in doc.select('.views-field-field-travel-number-date')[1:]]\n",
    "    numbers = [int(number.text.strip().replace(',', '')) for number in doc.select('.views-field-field-travel-number')[1:]]\n",
    "    archive.extend([{\"date\": date, \"number\": number} for (date, number) in zip(dates, numbers)])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1826"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get this year's data\n",
    "For the current year's data, the code is very simliar to before but the URL doesn't fit our previous pattern, so we do this step seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.tsa.gov/travel/passenger-volumes\"\n",
    "response = requests.get(url)\n",
    "doc = BeautifulSoup(response.text, 'html.parser')\n",
    "dates = [date.text.strip() for date in doc.select('.views-field-field-travel-number-date')[1:]]\n",
    "numbers = [int(number.text.strip().replace(',', '')) for number in doc.select('.views-field-field-travel-number')[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.extend([{\"date\": date, \"number\": number} for (date, number) in zip(dates, numbers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "And that's it! Congratulations. You have now scraped your first webpage. All that's left to do is feed the variable `archive` which we used to store the list of dictionaries into pandas and write it to a csv file `tsa_volumes.csv`.\n",
    "\n",
    "We will pick back up in `exploration.qmd` where we will use the R programming language to analyze this data and prototype a graphic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tsa_volumes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
